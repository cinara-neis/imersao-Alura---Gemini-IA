para esse titulo:
Como a análise de dados a partir de textos escritos pode aperfeiçoar o desempenho do setor industrial
esse texto:
Quando falamos sobre análise de dados, as pessoas costumam imaginar que as informações contidas nela estão atreladas apenas aos números. Porém, os textos escritos feito por meio de anotações de observação dos profissionais durante as atividades operacionais também são considerados “dados” e podem auxiliar no complemento de relatórios a fim de atribuir informações úteis de serem exploradas pela indústria futuramente.Um exemplo que pode ser usado para explicar sobre a relevância dessas observações é o de operações de perfuração. Geralmente costumam ser atividades custosas e, por esse motivo, o monitoramento contínuo delas torna-se necessário para reduzir as interrupções do processo e evitar possíveis falhas dos equipamentos. Para isso, os Relatórios Diários de Perfuração (DDRs) têm sido a principal maneira de registrar eventos perceptíveis, tendo em vista que o setor industrial teve um crescimento significativo de digitalização durante a última década. Nesse contexto, os DDRs passaram a conter uma valiosa base de informações para a maioria das empresas de petróleo, por se tratar de um relatório completo que relata os principais eventos ocorridos em uma plataforma de perfuração. Geralmente é escrito por operadores e constituído por uma rica fonte de informação. No entanto, a tarefa de extrair conhecimento desses relatórios também pode ser cara, demorada e propensa a erros humanos, por isso é necessário ter o auxílio de abordagens capazes de reconhecer os eventos de perfuração em DDRs: uma delas é o método de processamento de linguagem baseado em regras, chamado Fuzzy String Matching (FSM). Trata-se da aplicação do algoritmo FSM na busca de um conjunto de palavras-chave e frases-chave predefinidas para extrair possíveis eventos de Tempo Perdido Invisível de DDRs que possam indicar risco de falha ou de baixa eficiência operacional. Isso também é possível ao combinar o conhecimento de especialistas com técnicas de Processamento de Linguagem Natural (NLP) para encontrar expressões considerando erros de digitação e diferentes sufixos ou prefixos.Em termos práticos, pode-se dizer que, em toda a operação onde existe uma supervisão de dados, raramente haverá  a interpretação de um profissional em uma planilha, pois geralmente ele irá escrever suas anotações por meio de uma linguagem natural - não será por meio da linguagem analítica. É a partir desse cenário que a ferramenta de NLP ganha importância, uma vez que  evita a perda do caráter “analítico” dos dados, sendo responsável por estabelecer um padrão de texto por meio de uma linguagem para que a tecnologia ajude na  compreensão e extração das informações mais relevantes, transformando os  dados “não estruturados” para “estruturados” - ou seja, unindo informação com análise. Além das metodologias citadas acima, é necessária a utilização de uma métrica de distância, como, por exemplo, a Normalized Damerau-Levenshtein (NDL) distance, que,  basicamente, é uma comparação entre duas expressões, onde uma delas é considerada a correta e a outra é uma expressão similar, porém contém uma forma de escrita diferente.  Por meio dessa métrica e da Subject Matter Experts(SME) - que utiliza conhecimento de especialistas na área operacional para ajudar a estabelecer quais são os problemas que costumam se repetir e que precisam ser codificados - é possível corrigir os erros ortográficos e identificar palavras repetidas que indiquem erros recorrentes e que precisam ser eliminados,  a fim de aprimorar o funcionamentos das operações.Para exemplificar a importância dos recursos utilizados, destaco a aplicação em uma empresa de perfuração, que compilou dados de 392 registros DDR do mundo real usando um conjunto de seis frases-chave do evento ILT anotadas por especialistas no assunto. Os resultados mostraram que, em 116 relatórios marcados como normais, 92 registros foram identificados como possíveis eventos de ILT, o que representa, em horas, uma soma de 923 horas de tempo não produtivo. Portanto, esses resultados comprovam que a utilização de todos os métodos são primordiais para que informações relevantes não sejam descartadas e possam gerar dados significativos, caso sejam bem estruturados. Além disso, evidencio a importância da metodologia FSM e sua capacidade de identificar problemas para facilitar a busca de DDRs problemáticos a fim de otimizar o fluxo e o tempo de trabalho do setor industrial.
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo:
Entendendo Rede Neural Convolucional (CNN) – Deep Learning          

esse texto:
As redes neurais convolucionais são amplamente usadas para, por exemplo, classificação de imagens, detecção de objetos e reconhecimento facial. Tendo essas como algumas de suas principais possibilidades de aplicação, fica entendido o porquê dos grandes esforços investidos no desenvolvimento da CNN.E um artigo que te ajudará a entender como as máquinas decifram imagens é o “Compreensão da Rede Neural Convolucional (CNN) – Deep Learning”. Nele, o cientista de dados Prabhu Raghav explica alguns conceitos básicos de CNN e as etapas envolvidas neste processo.Raghav organiza seu texto nestes seis tópicos, cada um deles dedicado a explicar uma das etapas envolvidas no processamento das redes neurais convolucionais: camada de convolução, strides, preenchimento, não linearidade (ReLU), camada de pooling e camada totalmente conectada. 

---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: Entendendo self em Python – Código rápido
esse texto: Depois de começar a usar o Python, não há como escapar da palavra “self “. Ela é vista nas definições de métodos e na inicialização de variáveis. Mas entender a ideia por trás disso parece um pouco problemático. E é nisto que este artigo te ajudará, a ter uma ideia intuitiva do que é o self e como você deve usá-lo.Para isso, o autor, Ashan Priyadarshana, primeiro explora a criação de métodos de instância e classe em Python e, a partir deste quadro, aprofunda a explicação sobre o uso de self em Python.Ao final do texto você terá conseguido entender por que deve sempre usar self como o primeiro parâmetro de métodos de instância em Python e o que realmente acontece nos bastidores quando chamamos um método de instância.

para esse titulo: Aprendendo Python do zero
esse texto:Um dos grandes motivos para aprender Python é certamente sua versatilidade; podemos usá-lo em diferentes áreas, como ciência de dados, desenvolvimento web e machine learning.Diante disso, não é surpreendente a popularidade dessa linguagem de programação, que é adotada pelas também popularíssimas Google, Netflix, Quora, Pinterest e Spotify.Se assim como essas gigantes, você também quer codificar em Python, o artigo “Aprendendo Python do zero” te ajudará. Nesse texto, por meio de exemplos de aplicação prática, você entenderá a teoria que estrutura o ambiente Python e também como se dão algumas de suas funções essenciais.Veja o conteúdo básico sobre Python que você entenderá melhor com este artigo:Como as variáveis em Python funcionamComo funciona o controle de fluxo — condicionaisComo Python lida com loop, iteração, mais especificamente while e forComo listas funcionamDicionários, uma estrutura de dados de chave-valorComo iteramos sobre essas estruturas de dadosObjetos e ClassesAtributos como dados de um objetoMétodos como comportamento de um objetoUsando getters e setters em Python & decorator propertyEncapsulamento: ocultando informaçõesHerança: herdando comportamentos e características.
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: Além do Pandas: Spark, Dask, Vaex e outras tecnologias de big data lutando frente a frente
esse texto:
Ao enfrentar um novo problema de ciência de dados, uma das primeiras perguntas a fazer é qual tecnologia usar. E foi pensando nisso que Jonathan Alexander, que acompanha intensamente o universo das ferramentas de big data, realizou testes para comparar a performance de tecnologias de conversão de dados e, assim, escolher a mais adequada para exploração, limpeza e conversão de dados tabulares. O objetivo é comparar as tecnologias em suas APIs, desempenho e facilidade de uso.Alexander compartilhou os resultados e etapas desses testes em seu artigo no  Medium, “Beyond Pandas: Spark, Dask, Vaex and other big data technologies battling head to head”.O conjunto de dados escolhidos para os testes foi o “Billion Row Taxi”, de 100 GB. As ferramentas selecionadas para comparação foram Dask DataFrame, PySpark, Koalas, Vaex, Turicreate e Datatable.Acompanhe o artigo completo para entender como essas ferramentas se comportam em diferentes categorias, como complexidade de codificação e recursos. Conhecer as particularidades dessas tecnologias te ajudará a escolher a solução adequada para seus diferentes projetos de big data. 
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: Use o Docker e o Airflow para implantar seu fluxo de trabalho de Data Science
esse texto: Este artigo busca ajudar cientistas de dados a aprimorar sua compreensão de como seu código seria executado em um ambiente de produção.Assim, esta matéria é uma introdução, baseada no passo a passo para o uso do Airflow e do Docker, de como se pode articular a escrita de um código para o nível de produção. O autor explica os diferentes estágios em um fluxo de trabalho de data science e como projetar DAG e pipeline para um determinado problema. Além disso, também fornece uma visão geral de como usar a UI do Airflow.
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: Como analisar 100 GB de dados no seu laptop com Python
esse texto: É cada vez mais comum os cientistas de dados enfrentarem conjuntos de dados de 50 GB ou mesmo 500 GB. Algo importante sobre trabalhar com esse tipo de conjunto de dados é que eles são pequenos o suficiente para caber no disco rígido de seu laptop comum, mas muito grandes para caber na RAM. Portanto, eles já são complicados de abrir e inspecionar, quanto mais de explorar ou analisar.Neste artigo, Jovan Veljanoski apresenta uma nova abordagem para dados dessas dimensões: o Vaex. Essa é uma biblioteca de DataFrame de código aberto que permite a visualização, exploração e análise em conjuntos de dados tabulares que são tão grandes quanto seu disco rígido. Para fazer isso, o Vaex emprega conceitos como mapeamento de memória, algoritmos out-of-core eficientes e lazy evaluations.Esta matéria de introdução ao Vaex te ajudará a aliviar alguns dos seus problemas com dados dessa proporção, pelo menos quando se trata de conjuntos de dados tabulares.
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: 5 operações em conjuntos Python que você precisa saber
esse texto: Estruturas de dados são os blocos de construção para scripts Python. Eles mantêm ou contêm os dados de uma maneira específica para fazer com que os scripts funcionem com mais eficiência. Portanto, é crucial aprender a interagir com as estruturas de dados.Os conjuntos (sets) são uma das principais estruturas de dados em Python. E neste artigo são apresentadas as características dos sets e examinadas cinco operações comumente usadas nos conjuntos.
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: Aprenda Ciência de Dados em 10 anos – 3 lições de Peter Norvig (diretor de aprendizado de máquina da Google)
esse texto: A demanda industrial, acadêmica e governamental por profissionais de ciência de dados qualificados é enorme, e continua crescendo rapidamente. A contínua “corrida de dados” está, portanto, atraindo profissionais com diversas formações, como física, matemática, estatística, economia e engenharia.O mercado tem urgência por mais cientistas de dados, mas a formação desses profissionais exige muito tempo e esforço.Inspirado pelo texto de Peter Norving sobre quanto tempo leva para se tornar um especialista em programação, Benjamin Obi Tayo compartilha em “Teach Yourself Data Science in 10 Years” passos essenciais para dominar os fundamentos da ciência de dados.Ao longo dessa conversa, Benjamin Obi Tayo te ajudará a encontrar as respostas para duas perguntas que todos os iniciantes em data science devem se fazer:Quanto tempo leva para aprender os fundamentos da ciência de dados?Quais recursos usar para aprender ciência de dados?De forma geral, este artigo vem para nos lembrar que para tornar-se um cientista de dados é preciso tempo, esforço, energia, paciência e comprometimento. Ou seja, cuidado com aquela conversa de “aprenda ciência de dados em quatro semanas”.
---------------------------------------------------------------------------------------------------------------------------------------------

para esse titulo: 6 práticas recomendadas para tornar suas funções Python mais legíveis
esse texto:
Você já olhou para uma função que escreveu um mês antes e achou difícil entendê-la em até 3 minutos? Se sim, é hora de refatorar seu código. Se você levar mais de 3 minutos para entender seu próprio código, imagine quanto tempo isto tomará de seus colegas.Se você deseja que seu código seja reutilizável, ele deve ser legível.E em seu artigo sobre como escrever código limpo em Python, a cientista de dados Khuyen Tran detalha seis práticas para tornar as funções mais legíveis e mais fáceis de detectar erros. Ela diz que as funções devem ser curtas, fazer uma única coisa, conter código com o mesmo nível de abstração, ter menos de quatro argumentos, não ter duplicação e usar nomes descritivos.Acompanhe o texto completo para entender como utilizar essas práticas e escrever códigos python muito mais legíveis. 
---------------------------------------------------------------------------------------------------------------------------------------------

e para esse texto: Para tomar qualquer decisão sobre como lidar com um conjunto de dados, é preciso entender com qual tipo de dados você está trabalhando. E nisso Tarun Gupta te ajudará com seu artigo “Types of Data Sets in Data Science, Data Mining & Machine Learning”.Antes de tratar especificamente de cada tipo de dataset, Gupta explica dimensionalidade, esparsidade e resolução, que são características gerais dos datasets. A partir disso, ele destrincha os três tipos de conjuntos de dados: dados de registro (record data), dados baseados em gráficos (graph-based data) e dados ordenados (ordered data).   Esses foram os 10 artigos selecionados para nossa lista de hoje, mas não faltam excelentes conteúdos de data science no Medium, como é o caso das publicações que fazemos lá. Então, Acompanhe o Insight Lab no Medium.E quais outros artigos te ajudaram a ser um cientista de dados melhor? Compartilha aqui nos comentários com a gente e nossos leitores. 

qual seria o titulo?

resposta:

Sugestões de Títulos para o Texto:
Opção 1 (Foco nos Tipos de Dados):
Desvendando os Dados: Um Mergulho nos Diferentes Tipos de Datasets em Ciência de Dados
Opção 2 (Foco na Importância da Compreensão dos Dados):
A Base do Conhecimento: Entendendo os Tipos de Dados para o Sucesso em Ciência de Dados
Opção 3 (Foco na Aplicação dos Dados):
Do Registro ao Grafo: Explorando os Tipos de Dados para Machine Learning e Data Mining